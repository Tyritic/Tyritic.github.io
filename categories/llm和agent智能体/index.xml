<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLM和Agent智能体 on Tyritic</title>
        <link>https://Tyritic.github.io/categories/llm%E5%92%8Cagent%E6%99%BA%E8%83%BD%E4%BD%93/</link>
        <description>Recent content in LLM和Agent智能体 on Tyritic</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Tyritic</copyright><atom:link href="https://Tyritic.github.io/categories/llm%E5%92%8Cagent%E6%99%BA%E8%83%BD%E4%BD%93/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>检索增强生成（RAG）</title>
        <link>https://Tyritic.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/</link>
        <pubDate>Sun, 04 May 2025 19:29:50 +0800</pubDate>
        
        <guid>https://Tyritic.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/</guid>
        <description>&lt;h2 id=&#34;rag的业务场景&#34;&gt;RAG的业务场景
&lt;/h2&gt;&lt;p&gt;随着自然语言处理技术的发展，纯生成模型虽然可以生成流畅的文本，但是存在以下问题。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;知识的局限性&lt;/strong&gt; ：大模型自身的知识完全源于它的训练数据，基本都是构建于网络公开的数据，对于一些实时性的、非公开的或离线的数据是无法获取到的，这部分知识也就无从具备。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;幻觉问题&lt;/strong&gt;： 所有的 AI 大模型的底层原理都是基于数学概率，其模型输出实质上是一系列数值运算，大模型也不例外，所以它有时候会一本正经地胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;什么是rag&#34;&gt;什么是RAG
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;RAG（检索增强生成，Retrieval-Augmented Generation）&lt;/strong&gt; 是一种结合 &lt;strong&gt;信息检索（Retrieval）&lt;/strong&gt; 和 &lt;strong&gt;文本生成（Generation）&lt;/strong&gt; 的 AI 方法，主要用于 &lt;strong&gt;提高大语言模型（LLM）的知识覆盖范围和回答准确性&lt;/strong&gt;。&lt;strong&gt;个人理解为RAG 让 AI 在回答问题时，先从外部知识库中检索相关信息，再结合检索到的内容生成最终回答，而无需重新训练模型&lt;/strong&gt; 这样可以减少 AI 依赖自身训练数据的局限性，并提供最新、准确的信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;检索&lt;/strong&gt; ：基于用户的输入，从外部知识库中检索与查询相关的文本片段，通常使用向量化表示和向量数据库进行语义匹配。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;增强生成&lt;/strong&gt; ：将检索到的知识送给大语言模型以此来优化大模型的生成结果 ，使得大模型在生成更精确、更贴合上下文答案的同时，也能有效减少产生误导性信息的可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;rag的流程&#34;&gt;RAG的流程
&lt;/h2&gt;&lt;p&gt;RAG（检索增强生成）的完整流程可分为5个核心阶段&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据准备&lt;/strong&gt;：清洗文档、分块处理（如PDF转文本切片）；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;向量化&lt;/strong&gt;：使用嵌入模型（如BERT、BGE）将文本转为向量；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;索引存储&lt;/strong&gt;：向量存入数据库（如Milvus、Faiss、Elasticsearch）；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;检索增强&lt;/strong&gt;：用户提问向量化后检索相关文档；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成答案&lt;/strong&gt;：将检索结果与问题组合输入大模型生成回答。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://Tyritic.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/image-20250504194757791.png&#34;
	width=&#34;400&#34;
	height=&#34;680&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RAG的基本流程&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;58&#34;
		data-flex-basis=&#34;141px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以简化为三个阶段（索引-检索-生成）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Tyritic.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/image-20250504195224385.png&#34;
	width=&#34;870&#34;
	height=&#34;702&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;RAG阶段&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;123&#34;
		data-flex-basis=&#34;297px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;索引阶段&#34;&gt;索引阶段
&lt;/h3&gt;&lt;p&gt;在索引阶段，对原始文档进行解析，并将其拆分成多个较小的文本块。随后，这些文本块会通过嵌入模型进行向量化处理，生成的向量将被存储在向量数据库中，供后续检索使用。&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
   
  .notice {
    --title-color: #fff;
    --title-background-color: #6be;
    --content-color: #444;
    --content-background-color: #e7f2fa;
  }

  .notice.info {
    --title-background-color: #fb7;
    --content-background-color: #fec;
  }

  .notice.tip {
    --title-background-color: #5a5;
    --content-background-color: #efe;
  }

  .notice.warning {
    --title-background-color: #c33;
    --content-background-color: #fee;
  }

   
  @media (prefers-color-scheme: dark) {
    .notice {
      --title-color: #fff;
      --title-background-color: #069;
      --content-color: #ddd;
      --content-background-color: #023;
    }

    .notice.info {
      --title-background-color: #a50;
      --content-background-color: #420;
    }

    .notice.tip {
      --title-background-color: #363;
      --content-background-color: #121;
    }

    .notice.warning {
      --title-background-color: #800;
      --content-background-color: #400;
    }
  }

  body.dark .notice {
    --title-color: #fff;
    --title-background-color: #069;
    --content-color: #ddd;
    --content-background-color: #023;
  }

  body.dark .notice.info {
    --title-background-color: #a50;
    --content-background-color: #420;
  }

  body.dark .notice.tip {
    --title-background-color: #363;
    --content-background-color: #121;
  }

  body.dark .notice.warning {
    --title-background-color: #800;
    --content-background-color: #400;
  }

   
  .notice {
    width: 100%;  
    max-width: 600px;  
    padding: 18px;
    line-height: 24px;
    margin-bottom: 24px;
    border-radius: 4px;
    color: var(--content-color);
    background: var(--content-background-color);
  }

  .notice p:last-child {
    margin-bottom: 0;
  }

   
  .notice-title {
    margin: -18px -18px 12px;
    padding: 4px 18px;
    border-radius: 4px 4px 0 0;
    font-weight: 700;
    color: var(--title-color);
    background: var(--title-background-color);
  }

   
  .icon-notice {
    display: inline-flex;
    align-self: center;
    margin-right: 8px;
  }

  .icon-notice img,
  .icon-notice svg {
    height: 1em;
    width: 1em;
    fill: currentColor;
  }

  .icon-notice img,
  .icon-notice.baseline svg {
    top: 0.125em;
    position: relative;
  }
&lt;/style&gt;&lt;div
  class=&#34;notice tip&#34;
  
&gt;
  &lt;p class=&#34;notice-title&#34;&gt;
    &lt;span class=&#34;icon-notice baseline&#34;&gt;
      &lt;svg xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;300.5 134 300 300&#34;&gt;
  &lt;path d=&#34;M551.281 252.36c0-3.32-1.172-6.641-3.515-8.985l-17.774-17.578c-2.344-2.344-5.469-3.711-8.789-3.711-3.32 0-6.445 1.367-8.789 3.71l-79.687 79.493-44.141-44.14c-2.344-2.344-5.469-3.712-8.79-3.712-3.32 0-6.444 1.368-8.788 3.711l-17.774 17.579c-2.343 2.343-3.515 5.664-3.515 8.984 0 3.32 1.172 6.445 3.515 8.789l70.704 70.703c2.343 2.344 5.664 3.711 8.789 3.711 3.32 0 6.64-1.367 8.984-3.71l106.055-106.056c2.343-2.343 3.515-5.468 3.515-8.789ZM600.5 284c0 82.813-67.188 150-150 150-82.813 0-150-67.188-150-150 0-82.813 67.188-150 150-150 82.813 0 150 67.188 150 150Z&#34;/&gt;
&lt;/svg&gt;

    &lt;/span&gt;提示&lt;/p&gt;&lt;p&gt;嵌入模型，文本向量化，向量数据库&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;嵌入模型：&lt;strong&gt;将文本数据（如词汇、短语或句子）转换为数值向量（文本向量化）的工具&lt;/strong&gt; ，这些向量捕捉了文本的语义信息，可用于各种自然语言处理（NLP）任务。例如在识物探趣项目中使用嵌入模型为 &lt;strong&gt;bge-small-zh-v1.5&lt;/strong&gt;（向量维度为 &lt;strong&gt;512&lt;/strong&gt; ）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;文本向量化： &lt;strong&gt;将文本映射到高维空间中的点，使语义相似的文本在这个空间中距离较近。&lt;/strong&gt; 例如，“猫”和”狗”的向量可能会比”猫”和”汽车”的向量更接近。总而言之，&lt;strong&gt;语义相近的对象在向量空间中彼此邻近，而语义相异的对象则相距较远。&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;向量数据库：专门设计用来存储和管理嵌入向量的数据库系统。它可以将非结构化数据(如文本、图片、音频等)转换成高维向量的形式进行存储，并提供高效的相似性搜索功能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&#34;https://Tyritic.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/1728219277.jpg&#34;
	width=&#34;3518&#34;
	height=&#34;1136&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;文本向量化&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;309&#34;
		data-flex-basis=&#34;743px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;检索阶段&#34;&gt;检索阶段
&lt;/h3&gt;&lt;p&gt;将用户的查询通过嵌入模型转换成向量 ，以便与向量数据库中存储的知识相关的向量进行比对。通过语义相似度匹配，从向量数据库中找出最匹配的前 K 个数据。&lt;/p&gt;
&lt;h3 id=&#34;增强生成阶段&#34;&gt;增强生成阶段
&lt;/h3&gt;&lt;p&gt;系统将用户查询与检索到的相关文本块进行组合，通过提示工程（Prompt Engineering）设计适当的输入格式，将用户的查询内容和检索到的相关知识 一起嵌入到一个预设的提示词模板，将经过检索增强的提示词内容输入到大语言模型中，以此生成所需的输出。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Tyritic.github.io/p/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90rag/80d5236e087e619edced930b19fb653a.png&#34;
	width=&#34;2569&#34;
	height=&#34;776&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;增强生成阶段&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;331&#34;
		data-flex-basis=&#34;794px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;rag的优化策略&#34;&gt;RAG的优化策略
&lt;/h2&gt;&lt;h3 id=&#34;索引阶段的优化策略&#34;&gt;索引阶段的优化策略
&lt;/h3&gt;&lt;h4 id=&#34;分块策略&#34;&gt;分块策略
&lt;/h4&gt;&lt;p&gt;分块就是把原始长文本（比如一本书、一篇论文）拆成若干个 &lt;strong&gt;“小块”&lt;/strong&gt;（通常几百字到上千字，比如500-1000字），每个小块包含&lt;strong&gt;相对完整的语义单元&lt;/strong&gt;，比如一个段落、几个段落或一个小节。&lt;/p&gt;
&lt;p&gt;为什么需要分块&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;模型处理能力限制&lt;/strong&gt;：大语言模型一次能处理的文本长度有限，太长的文本塞进去会“消化不良”，分块后每个小块能塞进模型的“肚子”里。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;精准定位信息&lt;/strong&gt;：用户提问通常针对局部内容，分块后每个小块像“信息卡片”，检索时能快速找到最相关的卡片，避免在整本大书里“大海捞针”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;平衡上下文与效率&lt;/strong&gt;：小块既能保留足够上下文（比如前后句子的逻辑），又能让计算机高效存储和检索（小块的向量计算更快）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以分块就是将输入文档或大段文本切分成多个较小的、可控粒度的“块”，以便后续的向量化检索和生成模块高效调用与组合。&lt;/p&gt;
&lt;p&gt;常见的分块策略&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自然结构分块&lt;/strong&gt; ：按文档原有格式拆分，比如遇到标题、空行、章节编号、标点符号分块。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;固定大小分块&lt;/strong&gt; ：将文本按固定字符数、词数或 token 数等均匀切分，简单易实现。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;滑动窗口分块&lt;/strong&gt; ：在固定大小基础上为相邻的块保留一定重叠，以减少上下文出现断裂&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;递归分块&lt;/strong&gt; ：先按段落/章节粗分，超长部分再递归细化（按句子或空行），适合复杂文档。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;语义分块&lt;/strong&gt; ：用 NLP 模型（如 BERT、GPT）判断文本语义边界，确保每个块是完整的语义单元（比如一个论点、一个案例）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;选取合适的嵌入模型&#34;&gt;选取合适的嵌入模型
&lt;/h4&gt;&lt;p&gt;选择嵌入模型的时候主要考虑以下因素&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义准确性&lt;/strong&gt;：模型能否精准捕捉文本语义（比如长句理解、上下文关联、同义词区分），直接影响向量相似度计算的可靠性。
&lt;ul&gt;
&lt;li&gt;部分模型擅长短文本（如 Sentence-BERT），但长文本下会丢失上下文。&lt;/li&gt;
&lt;li&gt;通用模型（如 text-embedding-ada-002）在专业领域可能“词不达意”（比如“主诉”在医疗文本中是专有名词，通用模型可能理解为“主要诉求”）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型效率&lt;/strong&gt;：推理速度是否满足业务实时性要求（比如 QPS 高的场景不能用太大的模型），显存/内存占用是否适配硬件资源。
&lt;ul&gt;
&lt;li&gt;轻量级模型（如 MiniLM、DistilBERT）速度快（毫秒级/句），适合实时问答；&lt;/li&gt;
&lt;li&gt;重型模型（如 BERT-large、GPT-4 Embedding）速度慢（秒级/句），适合离线批量处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;领域适配&lt;/strong&gt;：是否针对垂直领域做过预训练或微调，原生支持特定术语和逻辑（比如金融模型懂“PE 估值”，通用模型可能误解为“体育器材”）。简而言之就是让模型懂行话
&lt;ul&gt;
&lt;li&gt;直接选领域专用模型&lt;/li&gt;
&lt;li&gt;用通用模型+领域数据微调（适合有私有语料的场景，比如用公司内部客服对话数据微调）&lt;/li&gt;
&lt;li&gt;添加领域适配器（如 LoRA 技术，在不改变原模型的前提下，新增少量参数适配领域）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多语言支持&lt;/strong&gt;：是否支持业务所需语言（尤其是小语种），以及跨语言对齐能力（比如中英混合文本能否正确嵌入）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据规模匹配&lt;/strong&gt;：模型参数量和训练数据规模是否匹配你的语料复杂度（小数据用大模型可能过拟合，大数据用小模型可能语义坍缩）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;检索阶段的优化策略&#34;&gt;检索阶段的优化策略
&lt;/h3&gt;&lt;h4 id=&#34;进行重排序&#34;&gt;进行重排序
&lt;/h4&gt;&lt;p&gt;Rerank 是一个对初步检索返回的候选文档列表进行再次排序的过程。因为初步检索需要&lt;strong&gt;快速地在海量的文档中&lt;/strong&gt;找出大致相关的文档，其需要考虑效率，所以查找出的文档不会非常准确，这步是&lt;strong&gt;粗排&lt;/strong&gt;。在已经筛选的相关的文档中再进行精筛，找出匹配度更高的文档让其排在前面，选其中的 Top-K 然后扔给大模型，提高答案的准确性，这就是 Rerank，也是&lt;strong&gt;精排&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;初步检索生成候选文档&lt;/strong&gt;：使用速度较快的传统检索方法获得一组候选文档。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;根据Rerank模型重新排序&lt;/strong&gt;：根据Rerank模型匹配得分对候选文档进行排序，选出最相关的 Top-K 文档。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交给生成模块&lt;/strong&gt;：Top-K 候选文档传递给大模型，帮助生成更精准、更富信息量的回答。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;增强生成阶段的优化策略&#34;&gt;增强生成阶段的优化策略
&lt;/h3&gt;&lt;h4 id=&#34;提示词优化&#34;&gt;提示词优化
&lt;/h4&gt;&lt;p&gt;设计合适的提示词可以提高RAG的效果&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;明确角色和任务&lt;/strong&gt;：提示中要清楚说明 AI 的身份、能力边界和目标任务。比如“你是一个专业法律助手，只能基于提供的材料回答问题”，可以有效限制模型的自由发挥。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结构化提示&lt;/strong&gt;：用明确的格式指导 AI 输出，比如：“背景 + 问题 + 输出格式”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加上下文约束&lt;/strong&gt;：明确告知 AI 只能基于检索到的资料回答，避免“幻觉”。
&lt;ul&gt;
&lt;li&gt;提示中明确“不要使用自己的知识推断”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加入冗余兜底机制&lt;/strong&gt;：如没检索到相关内容，就让 AI 显式回复 “未找到相关资料”
&lt;ul&gt;
&lt;li&gt;在 prompt 中加入 “如果找不到资料，请回复‘未找到相关内容’”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;查询重写&#34;&gt;查询重写
&lt;/h4&gt;&lt;p&gt;利用 LLM 从不同视角生成多个查询，优化用户输入&lt;/p&gt;
&lt;h4 id=&#34;多查询检索时进行重排序&#34;&gt;多查询检索时进行重排序
&lt;/h4&gt;&lt;p&gt;采用类似检索阶段的重排序策略&lt;/p&gt;
</description>
        </item>
        <item>
        <title>模型上下文协议（MCP）</title>
        <link>https://Tyritic.github.io/p/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/</link>
        <pubDate>Thu, 27 Mar 2025 10:00:39 +0800</pubDate>
        
        <guid>https://Tyritic.github.io/p/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/</guid>
        <description>&lt;h2 id=&#34;llm的一些缺陷&#34;&gt;LLM的一些缺陷
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;市面上的LLM大部分是基于网络上通用知识训练得来的，缺乏对垂直领域相关的知识，类似一个盲目自信的人对于不了解不理解的问题也会编造一些内容（ &lt;strong&gt;幻觉&lt;/strong&gt; 问题）&lt;/li&gt;
&lt;li&gt;LLM的知识不具备实时性，例如向LLM询问今天的天气时LLM无法给出正确答案&lt;/li&gt;
&lt;li&gt;LLM无法主动从网络等各种来源主动获取知识，即LLM目前只能作为 &lt;strong&gt;规划者&lt;/strong&gt; 而无法成为 &lt;strong&gt;执行者&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;传统解决方案函数调用&#34;&gt;传统解决方案——函数调用
&lt;/h2&gt;&lt;p&gt;Function-Call于2023年由OpenAI提出。在此之前，LLM 主要通过自然语言回答问题，无法直接执行 API 调用或插件交互。这一机制的核心在于  &lt;strong&gt;模型能够以结构化的 JSON 格式输出函数参数&lt;/strong&gt; 。&lt;/p&gt;
&lt;p&gt;工作流程如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型识别用户输入的意图判断是否需要使用Function-Call&lt;/li&gt;
&lt;li&gt;模型生成一个结构化的JSON对象来包装函数的相关参数并转发到应用程序的对应函数中&lt;/li&gt;
&lt;li&gt;应用程序执行函数并将结果返回给模型&lt;/li&gt;
&lt;li&gt;模型将应用程序返回的结果进行整理并以自然语言整理出最终回答、&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是 function call 也有其局限性，重点在于function call 平台依赖性强，不同 LLM 平台的 function call API 实现差异较大。例如，OpenAI 的函数调用方式与 Google 的不兼容，开发者在切换模型时需要重写代码，增加了适配成本。&lt;/p&gt;
&lt;h2 id=&#34;mcp协议是什么&#34;&gt;MCP协议是什么
&lt;/h2&gt;&lt;p&gt;MCP全称模型上下文协议，是由Claude公司推出的标准化LLM与外部数据源与工具的交互形式。跳转&lt;a class=&#34;link&#34; href=&#34;https://modelcontextprotocol.io/introduction&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官方文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;引用Claude对此的描述&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;MCP（模型上下文协议）是一种开放协议，它对应用程序向大语言模型（LLM）提供上下文的方式进行了标准化。可以把 MCP 想象成人工智能应用程序的 USB-C 接口。就像 USB-C 接口为将设备连接到各种外围设备和配件提供了一种标准化的方式一样，MCP 也为将人工智能模型连接到不同的数据源和工具提供了一种标准化的方式。&lt;/p&gt;
&lt;p&gt;MCP旨在为大型语言模型（LLMs）和 AI 助手提供一个统一、标准化的接口，使其能够无缝连接并交互各种外部数据源、工具等，让模型不依赖于预训练数据，还能在需要时动态获取最新的上下文信息、调用外部工具、执行特定任务。&lt;/p&gt;
&lt;p&gt;可以从以下角度理解MCP协议&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MCP协议类似于一个人工智能应用程序的拓展坞，通过拓展坞可以以标准化的形式将LLM连接到各种外部工具和数据源&lt;/li&gt;
&lt;li&gt;同时也可以将MCP协议理解为Function-Call的标准化和优化，MCP协议依然是建立在Function-Call的基础上的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MCP的作用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标准化数据接入&lt;/strong&gt;：通过 MCP，我们无需为每个模型编写单独的代码，而是通过统一的协议接口，实现一次集成，随处连接。这大大简化了模型与外部系统的集成过程。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;增强模型能力&lt;/strong&gt;：MCP 使得模型能够实时访问最新的数据和工具，例如直接从 GitHub 获取代码库信息，或从本地访问文件。这不仅提升了模型的实用性，也拓展了其应用场景。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提升系统可维护性&lt;/strong&gt;：通过标准化的协议，系统的各个组件可以更加模块化地协作，降低了维护成本和出错概率。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mcp协议相较于function-call的改进&#34;&gt;MCP协议相较于Function-Call的改进
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tool&lt;/strong&gt; ：基于Function-Call实现对外部工具的调用，使得模型可以进行具体的操作&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务器定义好工具（比如“计算两点距离”或者“发个邮件”），客户端发现这些工具后，LLM 就能根据需要调用。调用完结果会返回给 LLM，继续推理或者输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Resource&lt;/strong&gt; ：LLM可以读取的数据，比如文件内容、数据库查询结果或API的响应&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务器（MCP Server）把这些数据暴露出来，客户端（比如 LLM 应用）可以读取它们，然后塞进模型的上下文里去推理或者生成内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prompt&lt;/strong&gt; ：服务器提供给AI的预写消息或模板，帮助AI理解如何使用资源和工具&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务器定义好一堆 Prompt 模板（比如“写个产品描述”或者“调试错误”），客户端可以直接选一个，填入参数，然后丢给 LLM 执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mcp的相关概念&#34;&gt;MCP的相关概念
&lt;/h2&gt;&lt;h3 id=&#34;核心架构&#34;&gt;核心架构
&lt;/h3&gt;&lt;p&gt;MCP 遵循客户端-服务器架构，所有传输都使用 &lt;a class=&#34;link&#34; href=&#34;https://www.jsonrpc.org/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;JSON-RPC&lt;/a&gt; 2.0 来交换消息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Tyritic.github.io/p/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/77afbba7a4543387ddb1f1827dac0b71.png&#34;
	width=&#34;1512&#34;
	height=&#34;1040&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;核心架构&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;主机&lt;/strong&gt; ：启动连接的 LLM 应用程序&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;客户端&lt;/strong&gt; ：嵌入在主机内，在主机应用程序内与服务器保持一比一连接&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;服务器&lt;/strong&gt; ：连接到具体的数据源或工具，向客户端提供资源、工具和提示
&lt;ul&gt;
&lt;li&gt;资源（Resources）：类似文件的数据，可以被客户端读取，如 API 响应或文件内容。&lt;/li&gt;
&lt;li&gt;工具（Tools）：可以被 LLM 调用的函数（需要用户批准）。&lt;/li&gt;
&lt;li&gt;提示（Prompts）：预先编写的模板，帮助用户完成特定任务。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源&lt;/strong&gt; ：服务器可以访问到的资源
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本地资源&lt;/strong&gt; : MCP 服务器可以安全访问的计算机资源（数据库、文件、服务）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;远程资源&lt;/strong&gt; : MCP 服务器可以连接到的互联网上可用的资源（API）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MCP支持两者模式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标准输入输出（Stdio）模式：基于 &lt;code&gt;stdio&lt;/code&gt; 的实现是最常见的 MCP 客户端方案，它通过标准输入输出流与 MCP 服务器进行通信。这种方式简单直观，能够直接通过进程间通信实现数据交互，避免了额外的网络通信开销。特别适用于本地部署的 MCP 服务器，可以在同一台机器上启动 MCP 服务器进程，与客户端无缝对接。&lt;/li&gt;
&lt;li&gt;服务器发送事件（SSE）模式：这个方案相较于 &lt;code&gt;stdio&lt;/code&gt; 方式，&lt;code&gt;SSE&lt;/code&gt; 更适用于远程部署的 MCP 服务器，比如分布式系统或需要网络实时推送的场景。客户端可以通过标准 HTTP 协议与服务器建立连接，实现单向的实时数据推送。基于 SSE 的 MCP 服务器支持被多个客户端的远程调用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;工作流程&#34;&gt;工作流程
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://Tyritic.github.io/p/%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AEmcp/image-20250327140914648.png&#34;
	width=&#34;1186&#34;
	height=&#34;326&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;工作流程&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;363&#34;
		data-flex-basis=&#34;873px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;初始化&lt;/strong&gt;：主机应用程序启动并初始化MCP Client，每个MCP Client与一个服务器建立连接。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;功能协商&lt;/strong&gt;：MCP Client和MCP Server之间进行功能协商，确定它们可以相互提供哪些功能和服务。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;请求处理&lt;/strong&gt;：MCP Client根据用户请求或AI模型的需要，向服务器发送请求。服务器处理这些请求，并可能与本地或远程资源进行交互。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;构造Function-calling请求&lt;/strong&gt;：将用户的查询连同工具描述通过 Function calling发送给 LLM。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLM智能判断是否调用工具&lt;/strong&gt;：LLM 决定是否需要使用工具以及使用哪些工具。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;工具调用执行&lt;/strong&gt;：如果需要使用工具，MCP Client会通过MCP Server执行相应的工具调用。MCP Server 去实际跑工具的逻辑，并把结果返回给MCP Client。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型整合&lt;/strong&gt;：工具执行结果被传回 LLM，由模型将这个结果与原始用户问题、已有上下文等信息整合，生成最终的自然语言回应。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;响应返回&lt;/strong&gt;：MCP Client再将信息传递回主机应用程序。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;安全性设计&#34;&gt;安全性设计
&lt;/h3&gt;&lt;p&gt;MCP的安全性设计体现在以下方面&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;用户同意和控制&lt;/strong&gt;：所有模型对工具、资源和提示的访问请求都必须经过用户的授权。主机负责管理权限、提示用户批准，并在必要时阻止未经授权的访问。用户必须知道哪些数据是需要提供给大模型的，用户在授权使用之前应该了解每个工具的功能。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;隔离与沙箱机制&lt;/strong&gt;：MCP 的设计将实际工具调用封装在 MCP Server 内部，模型本身无法直接访问敏感数据，这种 “中间层”设计有效地降低了直接暴露内部业务系统的风险。同时，沙箱机制可以限制工具调用的执行环境，防止任意代码执行或恶意操作对系统造成破坏。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;加密传输与来源验证&lt;/strong&gt;：MCP 内置了安全机制，确保只有经过验证的请求才能访问特定资源，相当于在数据安全又加上了一道防线。同时，MCP协议还支持多种加密算法，以确保数据在传输过程中的安全性。&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
